---
title: "HarvardX Data Science Program"
subtitle: 'CYO Capstone Project - COVID19 Total Cases Global Forecasting'
author: "Le Cong Binh"
email: "binhlc@gmail.com"
date: "`r format(Sys.Date())`"
#header-includes:
#- \usepackage{titling}
#- \pretitle{\begin{center}\LARGE\includegraphics[width=6cm]{HarvardX.jpg}\LARGE\includegraphics[width=3cm]{edx.png}\\[\bigskipamount]\vspace{5cm}}
#- \posttitle{\end{center}}
#include-before:
#- '`\newpage{}`{=latex}'
output: 
  pdf_document: 
    number_sections: yes
    fig_caption: yes
    toc: yes
    fig_height: 3
    includes:
      #in_header: preamble.tex
    #latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

# Introduction

This is a report of HarvardX Data Science Program - CYO - Capstone Project (PH125.9x).

In this project, We will be predicting the cumulative number of confirmed COVID19 cases in various locations across the world, for future dates. To achieve our goal the project is divided in the following phases: Data exploration; Model identification; Build and improve the model; Limitations and conclusion.

Corona Virus are zoophytic viruses (means transmitted between animals and people).Symptoms include from fever, cough, respiratory symptoms, and breathing difficulties. In severe cases, it can cause pneumonia, severe acute respiratory syndrome (SARS), kidney failure and even death. Corona Virus are also asymptomatic, means a person can be a carrier for the infection but experiences no symptoms.

**Dataset:**

The data repository for the Novel Corona Virus operated by the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). Also, Supported by ESRI Living Atlas Team and the Johns Hopkins University Applied Physics Lab (JHU APL).

It is available here: https://github.com/CSSEGISandData/COVID-19

Before building a model we have to perform Exploratory data analysis (EDA) and select metric for model estimation. RMSE is our metric for this project. It can be calculated by equation: $$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{i} (\hat{y}_{i}-y_{i})^{2}}, $$where $N$ is size of test-set, $y_{i}$ is the true rating given by user $i$ day. Root mean squared error (RMSE) is reported in the same units as the outcomes, which makes understanding what is large and what is small enough RMSE more intuitive.

Final RMSE estimation will be performed on the final hold-out validation test set, which we will not use for any other purposes, neither for training model nor for model selection.

# Data preparation

```{r libraries, echo=TRUE, message=FALSE, warning=FALSE}
# loading libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(forecast)) install.packages("forecast", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
```

The Code bellow will download data from github

```{r covid-data-download, echo=TRUE, warning=FALSE, message=FALSE}
# download data
url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_confirmed <- read.csv(url)
url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'
df_death <- read.csv(url)
```
 
In the original data, a day stands for a variable(column), but they should be placed by row. So we have to get all the days together and create a variable “Date” to store them (per day per row format).
 
```{r covid-data-prepare, echo=TRUE, warning=FALSE, message=FALSE}
# Reshape data
df_confirmed <- df_confirmed %>%
  gather(key = Date, value = TotalCase,-Province.State,-Country.Region,-Lat,-Long)
df_confirmed$Date <- as.Date(strptime(substr(df_confirmed$Date,2,length(df_confirmed$Date)-1),format="%m.%d.%y"))
  
df_death <- df_death %>%
  gather(key = Date, value = TotalDeath,-Province.State,-Country.Region,-Lat,-Long)
df_death$Date <- as.Date(strptime(substr(df_death$Date,2,length(df_death$Date)-1),format="%m.%d.%y"))

df <- left_join(df_confirmed,df_death,by=c("Country.Region","Province.State","Lat","Long","Date"))
df <- df %>% group_by(Country.Region) %>% arrange(Date) %>%
  mutate(NewCase = (TotalCase - lag(TotalCase,1))) %>%
  mutate(NewDeath = (TotalDeath - lag(TotalDeath,1)))

```


# Exploratory data analysis

## First look on dataset

```{r class, echo=TRUE}
class(df)
```

Class of our dataset is data.frame, we can work with this data class as is. Let's see on first 6 records in the dataset:

```{r head, echo=FALSE}
# First look in dataset
knitr::kable(tail(df, 6), caption = "The last records of dataset ")
```

After data wrangling, now we have a dataset with: Province, Country, Lat, Long, Total cases, Total death, New cases, New death and Date. In this project We focus on total case forecast only.

We have `r n_distinct(df$Country.Region)` country and region; from `r min(df$Date)` to `r max(df$Date)`.

Now we look about statistics of total case group by country:

```{r case-stat, echo=TRUE}
summary(df %>% group_by(Country.Region) %>% summarise(TotalCase = sum(TotalCase)) %>% pull(TotalCase))
```

## Country and Date

Number of unique country in dataset: `r n_distinct(df$Country.Region)`; unique date in dataset: `r n_distinct(df$Date)`.

Plot of total case by date:

```{r case-distr, echo=TRUE, fig.cap = "Distibution of total cases", fig.align='center'}
# Total case by date
df %>% group_by (Date) %>% summarize(TotalCases = sum(TotalCase)) %>% 
    ggplot(aes(Date,TotalCases)) + 
    geom_point() +
    geom_smooth()
```

The trend of total case is fit with exponential smoothing. So, We should be use  some algorithm from forecast package.

## Country and total cases

First, let's look on the top-10 and bottom 10 movies:

```{r best-worst-country, echo=TRUE, message=FALSE}
# Top Country overview
knitr::kable(df %>% group_by(Country.Region) %>% 
  summarise(current_case = max(TotalCase)) %>% 
  arrange(desc(current_case)) %>% 
  ungroup() %>%
  head(10), 
  caption = "Top country by case")

df %>% group_by(Country.Region) %>% 
  summarise(current_case = max(TotalCase)) %>% 
  arrange(desc(current_case)) %>% head(10) %>%
  ggplot(aes(reorder(Country.Region, current_case), current_case, fill = current_case)) +
  geom_bar(stat = "identity") +
  coord_flip()

knitr::kable(df %>% group_by(Country.Region) %>% 
  summarise(current_case = max(TotalCase)) %>% 
  arrange(current_case) %>% 
  ungroup() %>%
  head(10), 
  caption = "Bottom country by case")
```

Looking on the tables, we see that the top-10 and bottom-10 have big different. The top country will be more effect to prediction results. This give us a ideal to predict on each of country. 

We will look the trend of total case in top country
```{r top-country-trend, echo=TRUE, warning=FALSE, message=FALSE}
# Top country trend
top_country <- df %>% group_by(Country.Region) %>% 
  summarise(current_case = max(TotalCase)) %>% 
  arrange(desc(current_case)) %>% 
  head(10)

inner_join(top_country, df %>% group_by(Date, Country.Region) %>% summarise(TotalCase = sum(TotalCase),.groups = 'drop'),by = "Country.Region") %>% ggplot(aes(Date,TotalCase,col = Country.Region)) + geom_line()
```

## Summary

The data is clean. Because the data set is limitation of feature so we will forecast using various methods, namely: naive approach, caret package (glm, knn, rf, ...) and forecast package (Holt linear, exponential smoothing, ARIMA,...)

We don't use strong tree base algorithm like Light GBM, XGB,.. or deep learning in this project.

# Methods of model building

In this chapter we will try different approaches to build prediction model.

## Validation technique


First, we need to create training and validation sets to train and validate our models. I will take the last 2 months as the validation data. 

```{r cov-split, echo=TRUE, warning=FALSE, message=FALSE}

# Train. test split
date_index <- max(df$Date) %m-% months(2)
train <- df %>% filter(Date <= date_index) %>% group_by(Date) %>% summarize(TotalCases = sum(TotalCase))
test <- df %>% filter(Date > date_index) %>% group_by(Date) %>% summarize(TotalCases = sum(TotalCase))

df %>% group_by (Date) %>% summarize(TotalCases = sum(TotalCase)) %>%
  mutate(group = ifelse(Date <= date_index,'Train','Test')) %>%
  ggplot(aes(Date,TotalCases,col = group)) + geom_line()

```
Check datasets dimensions: dimensions of dataset are `r dim(train)` and dimensions of validation dataset are `r dim(test)` .\

Function of the RMSE is defined by code:

```{r rmse_fun, echo=TRUE}
# function to estimate RMSE
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

## Linear model

In order to have some baseline, we will implement the simplest model. Assume, that the total case depend on Date only

```{r covid-linear}
# Base line model
fit <- lm(TotalCases ~ Date, data = train)
y_hat <- predict(fit,test)
rmse <- RMSE(test$TotalCases,y_hat)

```
With just predicting the linear model we have RMSE = `r rmse`. 
And save RMSE to the list:

```{r first-model}
rmse_results <- tibble(method = "lm", RMSE = rmse)
```

## Caret model selection

We will scan some basic model using caret package and save predict results

```{r caret-model}
# Caret model scan
set.seed(1, sample.kind="Rejection") 
models <- c("glm", "svmLinear", "knn", "rf")
fits <- lapply(models, function(model){
    print(model)
    train(TotalCases~Date, method = model, data =train)
    #y_hat <- predict(fit, newdata = test)
    #y_hat[is.na(y_hat)] = 0    
}) 

# save predict data
pred <- sapply(fits, function(object)
    predict(object, newdata = test))

pred[is.na(pred)] = 0

#Let show predict result on test set

caret_pred <- test %>% select(Date, TotalCases)
caret_pred <- cbind(caret_pred, pred)
colnames(caret_pred) <- c("Date","TotalCases",models)
knitr::kable(caret_pred %>% head(10), caption = "Top of result in caret model")

# Save RMSE result
rmse <- sapply(seq(1,length(models)), function(i) sqrt(mean((pred[,i] - test$TotalCases)^2)))
names(rmse) <- models

for (model in models){
    rmse_results <- bind_rows(rmse_results, tibble(method=model, RMSE = rmse[model] ))    
}

knitr::kable(rmse_results, caption = "RMSE of caret model")

```
The smallest RMSE is `r rmse_results[nrow(rmse_results),2]` with algorithm `r rmse_results[nrow(rmse_results),1]` It's seems very high. We will try with some algorithm in forecast package


## Forecast model selection
```{r forcast-model}
library(forecast)
forcast_model <- c("arima","ets","bats")
set.seed(2021)
forcast_pred <- sapply(forcast_model, function(model){
    y = train$TotalCases
    if (model == "arima"){
        fit <- auto.arima(y)
    } 
    else if (model == "ets"){
        fit <- ets(y)
    }
    else{
        fit <- tbats(y)        
    }
    y_hat <- forecast(fit, h=nrow(test)) %>% .$mean
    #rmse <- sqrt(mean((y_hat - test$TotalCases)^2))    
})
pred_result <- cbind(caret_pred,forcast_pred)
```

```{r forcast-rmse}
rmse_results <- sapply(3:ncol(pred_result), function(i){sqrt(mean((pred_result[,i] - pred_result$TotalCases)^2))})
names(rmse_results) <- colnames(pred_result)[3:ncol(pred_result)]
knitr::kable(tibble(Algorithm = names(rmse_results), RMSE=unlist(rmse_results)) %>% arrange(RMSE), caption = "RMSE includes forecast model")
```

## Model Selection
```{r model-selection}
# Model compare
pred_result %>%
    pivot_longer(!c('Date'), names_to = "algorithm", values_to = "y_hat") %>%
    ggplot(aes(Date,y_hat, col = algorithm)) + 
    geom_line() 

```
This result confirm the ideal the best model is exponential smooth. Because we don't have many parameter of est to tuning, we will try est for each of country to final tuning. 

## Predict by each of country

```{r country-model}
#Fine tuning for est
## Predict by Country
train_country <- df %>% filter(Date <= date_index) %>% 
    group_by(Country.Region,Date) %>%
    summarize(TotalCases = sum(TotalCase),.groups = 'drop')
#train_country

pred_country <- sapply(unique(train_country$Country.Region), function(country){
    # train with country have covid already
    y <- train_country %>% filter(Country.Region == country & TotalCases > 0) %>% pull(TotalCases)
    if (length(y) ==0){
        y_hat = 0
    } else if (length(y) < 100){
        y_hat = max(y)
    } else {
        fit <- ets(y)
        y_hat <- forecast(fit, h=nrow(test)) %>% .$mean
    }
    y_hat    
})

df_pred_country <- pred_country %>% as_tibble() %>% 
    mutate(Date = test$Date) %>%
    pivot_longer(!c('Date'), names_to = "Country.Region", values_to = "y_hat")

y_hat <- df_pred_country %>% group_by(Date) %>% summarize(y_hat = sum(y_hat)) %>% pull(y_hat)
#sqrt(mean((y_hat - test$TotalCases)^2))
```
Base on country the RMSE is `r RMSE(test$TotalCases,y_hat) ` It's not better than predict by all country. 

## Final Result

In a week training model We saw the Auto ARIMA and est have the similar RMSE. But all most of case the RMSE score of est algorithm is better when we update data daily. So the final model is est in forecast package for all country. 

### Recheck model on train / test set
```{r final-model-summary}
# Recheck model on train / test set
y_hat <- train %>% pull(TotalCases) %>% ets() %>% forecast(nrow(test))
rbind(train %>% mutate(Actual = TotalCases, Predict = TotalCases) %>% select(Date, Actual, Predict),
test %>% mutate(Actual = TotalCases, Predict = y_hat$mean) %>% select(Date, Actual, Predict)) %>%
  ggplot(aes(x = Date)) +
  geom_line(aes(y = Predict, col = 'Predict')) +
  geom_line(aes(y = Actual, col = 'Actual'))+ 
  scale_color_manual(name = "", values = c("Actual" = "darkblue", "Predict" = "red"))
```

### Final prediction for next month
The model predict lower than actual but the chart is smoothly. So we have some ideal to choose the range to predict in the future (in this case we will choose the higher band of final forecast). Let's rerun model for all data to forecast the total case in next month

```{r final-result-summary}

finalResult <- df %>% group_by(Date) %>% summarize(TotalCases = sum(TotalCase)) %>% 
  pull(TotalCases) %>%
  ets() %>% forecast(10) %>% summary() %>% as_tibble() %>%
  mutate(Date = max(df$Date))

finalResult$Date <- seq(max(df$Date)+1,max(df$Date)+10,by="days")
knitr::kable(finalResult %>% select('Date','Point Forecast','Lo 80','Hi 80','Lo 95','Hi 95'), caption = "Next 10 days  global total cases forecast ")
  
```

```{r final-result}
df %>% group_by(Date) %>% summarize(TotalCases = sum(TotalCase)) %>% 
  pull(TotalCases) %>%
  ets() %>% forecast(60) %>% 
  autoplot()
```

# Conclusion

In this project we have built a model to predict global total case of corona virus. We scan many basic model and found that the best algorithm is exponential smooth. This dataset don't have many feature, that need complex machine learning algorithm like XGB, Light GBM or Cat-boots or Deep learning model. 

Possible future development of the model can be:

-   Try some booting tree with lag and average feature from time series

-   Use deep learning algorithms: LSTM or basic network;

This project only shows my skill about Data Visualization, Data Wrangling, Data Modeling that I learnt in this program. It also gives the model to define the trend of covid 19 in the world. To predict Corona Pandemic in fact We need many job to do: Additional reference data: vaccinate rate, population per country, the policy about tourist, transportation in each country, ... Which need much time to complete. So, this is the limitation of project. 


\newpage

# Literature

1.  [Rafael A. Irizarry, Introduction to Data Science](https://rafalab.github.io/dsbook/)
2.  [HarvardX Data Science Program](https://courses.edx.org/dashboard/programs/3c32e3e0-b6fe-4ee4-bd4f-210c6339e074/)
